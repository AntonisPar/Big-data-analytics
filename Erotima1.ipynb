{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alert-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import from_unixtime, hour, dayofyear\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "from sparkmeasure import StageMetrics\n",
    "import numpy as np\n",
    "import random\n",
    "from pyspark.sql.functions import broadcast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName('Vaseis2')\\\n",
    ".config(\"spark.jars\", \"<path-to-jar>/spark-measure_2.12-0.17.jar\") \\\n",
    ".master(\"spark://antonis:7077\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "stagemetrics = StageMetrics(spark)\n",
    "\n",
    "\n",
    "movies = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"movie.csv\") #filename to read from\n",
    "     )\n",
    "\n",
    "ratings = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"rating.csv\") #filename to read from\n",
    "     )\n",
    "\n",
    "\n",
    "tag = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"tag.csv\") #filename to read from\n",
    "     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunset-documentary",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22243\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "numTasks => 214\n",
      "elapsedTime => 3460 (3 s)\n",
      "stageDuration => 5964 (6 s)\n",
      "executorRunTime => 38088 (38 s)\n",
      "executorCpuTime => 28765 (29 s)\n",
      "executorDeserializeTime => 793 (0.8 s)\n",
      "executorDeserializeCpuTime => 394 (0.4 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 3075 (3 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 532 (0.5 s)\n",
      "resultSize => 994163 (970.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 1439170560\n",
      "recordsRead => 20000264\n",
      "bytesRead => 693171282 (661.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 20000464\n",
      "shuffleTotalBlocksFetched => 2601\n",
      "shuffleLocalBlocksFetched => 2601\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 70268602 (67.0 MB)\n",
      "shuffleLocalBytesRead => 70268602 (67.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 70268602 (67.0 MB)\n",
      "shuffleRecordsWritten => 20000464\n"
     ]
    }
   ],
   "source": [
    "#QUERY 1\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "merged = movies.join(ratings, on=['movieId'], how='left_outer')\n",
    "\n",
    "seen_jumanji = merged\\\n",
    ".filter(F.col(\"title\") == \"Jumanji (1995)\").count()\n",
    "\n",
    "print(seen_jumanji)\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "downtown-chinese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|(500) Days of Sum...|\n",
      "|101 Reykjavik (10...|\n",
      "|12 Years a Slave ...|\n",
      "|         1408 (2007)|\n",
      "|1492: Conquest of...|\n",
      "|2001: A Space Ody...|\n",
      "|2010: The Year We...|\n",
      "|         2046 (2004)|\n",
      "|     21 Grams (2003)|\n",
      "|24 Hour Party Peo...|\n",
      "|3-Iron (Bin-jip) ...|\n",
      "|40-Year-Old Virgi...|\n",
      "|    6 Bullets (2012)|\n",
      "| 633 Squadron (1964)|\n",
      "| 7 Plus Seven (1970)|\n",
      "|      8 Women (2002)|\n",
      "|A.I. Artificial I...|\n",
      "|  About a Boy (2002)|\n",
      "|According to Gret...|\n",
      "|   Adaptation (2002)|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 207\n",
      "elapsedTime => 313 (0.3 s)\n",
      "stageDuration => 283 (0.3 s)\n",
      "executorRunTime => 740 (0.7 s)\n",
      "executorCpuTime => 592 (0.6 s)\n",
      "executorDeserializeTime => 389 (0.4 s)\n",
      "executorDeserializeCpuTime => 271 (0.3 s)\n",
      "resultSerializationTime => 4 (4 ms)\n",
      "jvmGCTime => 47 (47 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 18 (18 ms)\n",
      "resultSize => 1404146 (1371.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 459276288\n",
      "recordsRead => 28366\n",
      "bytesRead => 23546842 (22.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 922\n",
      "shuffleTotalBlocksFetched => 628\n",
      "shuffleLocalBlocksFetched => 628\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 66137 (64.0 KB)\n",
      "shuffleLocalBytesRead => 66137 (64.0 KB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 66137 (64.0 KB)\n",
      "shuffleRecordsWritten => 922\n"
     ]
    }
   ],
   "source": [
    "#QUERY 2\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')\n",
    "\n",
    "boring = tags_and_movies\\\n",
    ".select('title')\\\n",
    ".where(tags_and_movies['tag'].contains(\"boring\")).dropDuplicates()\n",
    "\n",
    "boring = boring\\\n",
    ".orderBy(col('title'))\n",
    "\n",
    "boring.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "exposed-quality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "| 93037|\n",
      "| 37355|\n",
      "| 20388|\n",
      "|    65|\n",
      "|  6500|\n",
      "| 50616|\n",
      "| 53397|\n",
      "| 20066|\n",
      "|124998|\n",
      "| 52698|\n",
      "|124139|\n",
      "|131829|\n",
      "| 54586|\n",
      "| 68558|\n",
      "| 19837|\n",
      "|  8513|\n",
      "| 77137|\n",
      "| 35170|\n",
      "| 41165|\n",
      "| 25004|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 6\n",
      "numTasks => 118\n",
      "elapsedTime => 2243 (2 s)\n",
      "stageDuration => 2213 (2 s)\n",
      "executorRunTime => 23505 (24 s)\n",
      "executorCpuTime => 21982 (22 s)\n",
      "executorDeserializeTime => 222 (0.2 s)\n",
      "executorDeserializeCpuTime => 133 (0.1 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 567 (0.6 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 20 (20 ms)\n",
      "resultSize => 325324 (317.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 69992448\n",
      "recordsRead => 12195701\n",
      "bytesRead => 713730828 (680.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 32\n",
      "shuffleTotalBlocksFetched => 31\n",
      "shuffleLocalBlocksFetched => 31\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 1836 (1836 Bytes)\n",
      "shuffleLocalBytesRead => 1836 (1836 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 3790 (3.0 KB)\n",
      "shuffleRecordsWritten => 66\n"
     ]
    }
   ],
   "source": [
    "#QUERY 3\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')\n",
    "\n",
    "\n",
    "# bollywood= tags_and_ratings\\\n",
    "# .select('userId')\\\n",
    "# .where(F.col('tag').contains(\"bollywood\")).distinct()\n",
    "\n",
    "\n",
    "bollywood= tags_and_ratings\\\n",
    ".select('userId')\\\n",
    ".where((F.col('tag').like(\"%bollywood\") | F.col('tag').like(\"%BOLLYWOOD\") | F.col('tag').like(\"%Bollywood\")) & (F.col('rating') > 3)).distinct()\n",
    "\n",
    "bollywood.sort('userId')\n",
    "          \n",
    "bollywood.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tamil-aside",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+\n",
      "|               title|Year|avg_rating|\n",
      "+--------------------+----+----------+\n",
      "|   Dancemaker (1998)|2005|       5.0|\n",
      "|Fear Strikes Out ...|2005|       5.0|\n",
      "|Gate of Heavenly ...|2005|       5.0|\n",
      "|Life Is Rosy (a.k...|2005|       5.0|\n",
      "|Married to It (1991)|2005|       5.0|\n",
      "|My Life and Times...|2005|       5.0|\n",
      "|Not Love, Just Fr...|2005|       5.0|\n",
      "|Paris Was a Woman...|2005|       5.0|\n",
      "|Take Care of My C...|2005|       5.0|\n",
      "|Too Much Sleep (1...|2005|       5.0|\n",
      "+--------------------+----+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 6\n",
      "numTasks => 813\n",
      "elapsedTime => 4093 (4 s)\n",
      "stageDuration => 4030 (4 s)\n",
      "executorRunTime => 39560 (40 s)\n",
      "executorCpuTime => 33634 (34 s)\n",
      "executorDeserializeTime => 1772 (2 s)\n",
      "executorDeserializeCpuTime => 1168 (1 s)\n",
      "resultSerializationTime => 34 (34 ms)\n",
      "jvmGCTime => 1463 (1 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 2277 (2 s)\n",
      "resultSize => 1745603 (1704.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 471859200\n",
      "recordsRead => 20027541\n",
      "bytesRead => 693171282 (661.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 173686\n",
      "shuffleTotalBlocksFetched => 12321\n",
      "shuffleLocalBlocksFetched => 12321\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 8058228 (7.0 MB)\n",
      "shuffleLocalBytesRead => 8058228 (7.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 4638143 (4.0 MB)\n",
      "shuffleRecordsWritten => 95352\n"
     ]
    }
   ],
   "source": [
    "#QUERY 4\n",
    "\n",
    "stagemetrics.begin()\n",
    "# top_rated = ratings\\\n",
    "# .groupBy(\"movieId\",ratings['timestamp'].substr(1,4))\\\n",
    "# .agg(avg(col(\"rating\")))\\\n",
    "# .withColumnRenamed(\"avg(rating)\", \"avg_rating\")\\\n",
    "# .sort(desc(\"avg_rating\"))\n",
    "\n",
    "# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)\n",
    "# top_rated_movies.show(10)\n",
    "\n",
    "query4 = ratings.join(movies,on=['movieId'], how ='left_outer')\n",
    "\n",
    "top_rated = query4\\\n",
    ".groupBy(\"title\",ratings['timestamp'].substr(1,4).alias(\"Year\"))\\\n",
    ".agg(avg(col(\"rating\")))\\\n",
    ".withColumnRenamed(\"avg(rating)\", \"avg_rating\")\\\n",
    ".sort(desc(\"avg_rating\"))\n",
    "\n",
    "\n",
    "\n",
    "def get_topN(df, group_by_columns, order_by_column, n=10):\n",
    "    window_group_by_columns = Window.partitionBy(group_by_columns)\n",
    "    ordered_df = df.select(df.columns + [\n",
    "        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])\n",
    "    topN_df = ordered_df.filter(f\"row_rank <= {n}\").drop(\"row_rank\")\n",
    "    return topN_df\n",
    "\n",
    "final_top_rated = get_topN(top_rated, top_rated[\"Year\"],top_rated[\"avg_rating\"], 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = final_top_rated\\\n",
    ".filter(F.col(\"Year\") == \"2005\")\\\n",
    ".orderBy(F.col(\"title\").asc())\n",
    "\n",
    "\n",
    "results.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fuzzy-gabriel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|movieId|                 tag|               title|              genres|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|  51372|                BD-R|\"\"Great Performan...|             Musical|\n",
      "|   2072|              1980's|  'burbs, The (1989)|              Comedy|\n",
      "|   2072|         dark comedy|  'burbs, The (1989)|              Comedy|\n",
      "|   2072|           Joe Dante|  'burbs, The (1989)|              Comedy|\n",
      "|   2072|        black comedy|  'burbs, The (1989)|              Comedy|\n",
      "|   2072|              quirky|  'burbs, The (1989)|              Comedy|\n",
      "|  69757|          depressing|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|              stupid|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|            artistic|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|           nonlinear|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|     no happy ending|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|         intelligent|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|           overrated|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|              boring|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|     Zooey Deschanel|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|           nonlinear|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|         bittersweet|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|Joseph Gordon-Levitt|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|        bad dialogue|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|                slow|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|            humorous|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|              quirky|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|               Funny|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|           nonlinear|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|         intelligent|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|               music|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|         bittersweet|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|            annoying|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|            artistic|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|              quirky|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|Joseph Gordon-Levitt|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|       relationships|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|             romance|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|               humor|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|            artistic|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|     Zooey Deschanel|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|             romance|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|           overrated|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  69757|Joseph Gordon-Levitt|(500) Days of Sum...|Comedy|Drama|Romance|\n",
      "|  26216|                BD-R|...tick... tick.....|        Action|Drama|\n",
      "| 128878|             Sukumar|            1 (2014)|Action|Mystery|Ro...|\n",
      "|   2572|              Cliché|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|               sunny|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|      clever writing|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|         chick flick|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|              clever|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|         chick flick|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|        Heath Ledger|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|                teen|10 Things I Hate ...|      Comedy|Romance|\n",
      "|   2572|       coming of age|10 Things I Hate ...|      Comedy|Romance|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 2\n",
      "numTasks => 7\n",
      "elapsedTime => 200 (0.2 s)\n",
      "stageDuration => 169 (0.2 s)\n",
      "executorRunTime => 616 (0.6 s)\n",
      "executorCpuTime => 563 (0.6 s)\n",
      "executorDeserializeTime => 7 (7 ms)\n",
      "executorDeserializeCpuTime => 9 (9 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 50 (50 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "resultSize => 1250834 (1221.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 31457280\n",
      "recordsRead => 492839\n",
      "bytesRead => 23546842 (22.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 0\n",
      "shuffleTotalBlocksFetched => 0\n",
      "shuffleLocalBlocksFetched => 0\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 0 (0 Bytes)\n",
      "shuffleLocalBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsWritten => 0\n"
     ]
    }
   ],
   "source": [
    "#QUERY 5\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "tags = tag\\\n",
    ".select(\"movieId\",\"tag\")\\\n",
    ".where(col('timestamp').substr(1,4) == '2015')\n",
    "\n",
    "tags_and_movies = tags.join(movies, on=['movieId'])\n",
    "\n",
    "\n",
    "tags_and_movies = tags_and_movies\\\n",
    ".sort(col(\"title\").asc())\n",
    "\n",
    "tags_and_movies.show(50)\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-position",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#QUERY 6\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "ratings_count = ratings\\\n",
    ".groupBy(\"movieId\")\\\n",
    ".agg(count(\"userId\"))\\\n",
    ".withColumnRenamed(\"count(userId)\", \"num_ratings\")\\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)\n",
    "\n",
    "\n",
    "ratings_per_movie = ratings_per_movie\\\n",
    ".sort(desc(\"num_ratings\"))\n",
    "\n",
    "ratings_per_movie.show(20, truncate=False)\n",
    "\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "express-papua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|userId|Year|counts|\n",
      "+------+----+------+\n",
      "| 28507|1995|     1|\n",
      "|131160|1995|     3|\n",
      "+------+----+------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 5\n",
      "numTasks => 614\n",
      "elapsedTime => 3528 (4 s)\n",
      "stageDuration => 3490 (3 s)\n",
      "executorRunTime => 37084 (37 s)\n",
      "executorCpuTime => 31070 (31 s)\n",
      "executorDeserializeTime => 1402 (1 s)\n",
      "executorDeserializeCpuTime => 815 (0.8 s)\n",
      "resultSerializationTime => 6 (6 ms)\n",
      "jvmGCTime => 1770 (2 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 38 (38 ms)\n",
      "resultSize => 1564090 (1527.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 54525952\n",
      "recordsRead => 20000263\n",
      "bytesRead => 691677634 (659.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 624 (624 Bytes)\n",
      "shuffleLocalBytesRead => 624 (624 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 468 (468 Bytes)\n",
      "shuffleRecordsWritten => 6\n"
     ]
    }
   ],
   "source": [
    "#QUERY 7\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "most_ratings = ratings\\\n",
    ".groupBy(\"userId\",ratings['timestamp'].substr(1,4).alias(\"Year\"))\\\n",
    ".agg(count(col(\"rating\")))\\\n",
    ".withColumnRenamed(\"count(rating)\", \"counts\")\\\n",
    ".orderBy(desc(\"counts\"))\n",
    "\n",
    "\n",
    "def get_topN(df, group_by_columns, order_by_column, n=10):\n",
    "    window_group_by_columns = Window.partitionBy(group_by_columns)\n",
    "    ordered_df = df.select(df.columns + [\n",
    "        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])\n",
    "    topN_df = ordered_df.filter(f\"row_rank <= {n}\").drop(\"row_rank\")\n",
    "    return topN_df\n",
    "\n",
    "final_most_ratings = get_topN(most_ratings, most_ratings[\"Year\"],most_ratings[\"counts\"], 10)\n",
    "\n",
    "results = final_most_ratings\\\n",
    ".filter(F.col(\"Year\") == \"1995\")\\\n",
    ".sort(col('userId').asc())\n",
    "\n",
    "results.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ambient-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|userId|Year|counts|\n",
      "+------+----+------+\n",
      "|135090|2012|  2101|\n",
      "| 76630|2012|  2085|\n",
      "| 98420|2012|  2044|\n",
      "| 13064|2012|  1749|\n",
      "|  3284|2012|  1742|\n",
      "|104063|2012|  1732|\n",
      "| 40483|2012|  1723|\n",
      "| 12644|2012|  1703|\n",
      "| 50367|2012|  1686|\n",
      "|102911|2012|  1570|\n",
      "|101044|2014|  2505|\n",
      "|137277|2014|  2387|\n",
      "| 91349|2014|  2159|\n",
      "| 54107|2014|  2018|\n",
      "| 37253|2014|  1985|\n",
      "| 93152|2014|  1888|\n",
      "|123352|2014|  1841|\n",
      "| 97860|2014|  1530|\n",
      "| 30317|2014|  1353|\n",
      "| 74222|2014|  1343|\n",
      "+------+----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+--------------------+-----------+\n",
      "|            genres|               title|ratings_num|\n",
      "+------------------+--------------------+-----------+\n",
      "|             Crime| Pulp Fiction (1994)|      67310|\n",
      "|           Romance| Forrest Gump (1994)|      66172|\n",
      "|          Thriller| Pulp Fiction (1994)|      67310|\n",
      "|         Adventure|Jurassic Park (1993)|      59715|\n",
      "|             Drama| Pulp Fiction (1994)|      67310|\n",
      "|               War| Forrest Gump (1994)|      66172|\n",
      "|       Documentary|Bowling for Colum...|      12280|\n",
      "|           Fantasy|    Toy Story (1995)|      49695|\n",
      "|           Mystery|Usual Suspects, T...|      47006|\n",
      "|           Musical|      Aladdin (1992)|      41842|\n",
      "|         Animation|    Toy Story (1995)|      49695|\n",
      "|         Film-Noir|L.A. Confidential...|      26836|\n",
      "|(no genres listed)|Doctor Who: The T...|         36|\n",
      "|              IMAX|    Apollo 13 (1995)|      47777|\n",
      "|            Horror|Silence of the La...|      63299|\n",
      "|           Western|Dances with Wolve...|      44208|\n",
      "|            Comedy| Pulp Fiction (1994)|      67310|\n",
      "|          Children|    Toy Story (1995)|      49695|\n",
      "|            Action|Jurassic Park (1993)|      59715|\n",
      "|            Sci-Fi|Jurassic Park (1993)|      59715|\n",
      "+------------------+--------------------+-----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 19\n",
      "numTasks => 1565\n",
      "elapsedTime => 14457 (14 s)\n",
      "stageDuration => 14479 (14 s)\n",
      "executorRunTime => 160246 (2.7 min)\n",
      "executorCpuTime => 125234 (2.1 min)\n",
      "executorDeserializeTime => 2889 (3 s)\n",
      "executorDeserializeCpuTime => 2022 (2 s)\n",
      "resultSerializationTime => 5 (5 ms)\n",
      "jvmGCTime => 23629 (24 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 8214 (8 s)\n",
      "resultSize => 3186104 (3.0 MB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 1853358080\n",
      "recordsRead => 40027804\n",
      "bytesRead => 1384848916 (1320.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 20920211\n",
      "shuffleTotalBlocksFetched => 118428\n",
      "shuffleLocalBlocksFetched => 118428\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 138240604 (131.0 MB)\n",
      "shuffleLocalBytesRead => 138240604 (131.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 131479830 (125.0 MB)\n",
      "shuffleRecordsWritten => 20796130\n"
     ]
    }
   ],
   "source": [
    "#QUERY 8\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "\n",
    "movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')\n",
    "\n",
    "\n",
    "genres = movies_and_ratings.withColumn(\"genres\",explode(split(\"genres\",\"[|]\")))\n",
    "\n",
    "\n",
    "FinalGenres = genres\\\n",
    ".groupby(\"genres\",\"title\")\\\n",
    ".agg(count(col(\"rating\")))\\\n",
    ".withColumnRenamed(\"count(rating)\", \"num_ratings\")\\\n",
    ".sort(desc(\"num_ratings\"))\n",
    "\n",
    "# FinalGenres.show(2000)\n",
    "\n",
    "TheFinalGenres = FinalGenres\\\n",
    ".groupby(\"genres\",\"title\")\\\n",
    ".agg(max(col(\"num_ratings\")))\\\n",
    ".withColumnRenamed(\"max(num_ratings)\",\"ratings_num\")\\\n",
    ".sort(desc(\"ratings_num\"))\n",
    "\n",
    "def retrieve_topN(df, group_by_columns, order_by_column, n):\n",
    "    window_group_by_columns = Window.partitionBy(group_by_columns)\n",
    "    ordered_df = df.select(df.columns + [\n",
    "        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])\n",
    "    topN_df = ordered_df.filter(f\"row_rank <= {n}\").drop(\"row_rank\")\n",
    "    return topN_df\n",
    "\n",
    "\n",
    "final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres[\"genres\"],TheFinalGenres[\"ratings_num\"], 1)\n",
    "\n",
    "final_most_ratings_genres = final_most_ratings_genres\\\n",
    ".sort(col('genres').asc())\n",
    "\n",
    "final_most_ratings_genres.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "legitimate-pension",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(number of users)|\n",
      "+--------------------+\n",
      "|                4322|\n",
      "+--------------------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 213\n",
      "elapsedTime => 7481 (7 s)\n",
      "stageDuration => 7479 (7 s)\n",
      "executorRunTime => 86480 (1.4 min)\n",
      "executorCpuTime => 67575 (1.1 min)\n",
      "executorDeserializeTime => 398 (0.4 s)\n",
      "executorDeserializeCpuTime => 329 (0.3 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 10140 (10 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 646 (0.6 s)\n",
      "resultSize => 840670 (820.0 KB)\n",
      "diskBytesSpilled => 236498117 (225.0 MB)\n",
      "memoryBytesSpilled => 2318401536 (2.0 GB)\n",
      "peakExecutionMemory => 2516582400\n",
      "recordsRead => 20000263\n",
      "bytesRead => 691677634 (659.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 20000244\n",
      "shuffleTotalBlocksFetched => 2600\n",
      "shuffleLocalBlocksFetched => 2600\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 344669784 (328.0 MB)\n",
      "shuffleLocalBytesRead => 344669784 (328.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 344669784 (328.0 MB)\n",
      "shuffleRecordsWritten => 20000244\n"
     ]
    }
   ],
   "source": [
    "#QUERY 9\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df4 = ratings\\\n",
    ".groupby(\"timestamp\",\"movieId\")\\\n",
    ".agg(count(col(\"userId\")))\\\n",
    ".withColumnRenamed(\"count(userId)\", \"number of users\")\\\n",
    ".filter(F.col(\"number of users\") != 1)\\\n",
    ".sort(desc(\"number of users\"))\n",
    "\n",
    "\n",
    "df5 = df4\\\n",
    ".select(sum(\"number of users\"))\n",
    "\n",
    "\n",
    "\n",
    "df5.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convenient-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|     genres|number of movies|\n",
      "+-----------+----------------+\n",
      "|     Action|             123|\n",
      "|  Adventure|             128|\n",
      "|  Animation|              83|\n",
      "|   Children|              97|\n",
      "|     Comedy|             527|\n",
      "|      Crime|              82|\n",
      "|Documentary|              15|\n",
      "|      Drama|             182|\n",
      "|    Fantasy|              78|\n",
      "|  Film-Noir|               3|\n",
      "|     Horror|              44|\n",
      "|       IMAX|              22|\n",
      "|    Musical|              39|\n",
      "|    Mystery|              20|\n",
      "|    Romance|             138|\n",
      "|     Sci-Fi|              60|\n",
      "|   Thriller|              69|\n",
      "|        War|              12|\n",
      "|    Western|               9|\n",
      "+-----------+----------------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 5\n",
      "numTasks => 419\n",
      "elapsedTime => 3654 (4 s)\n",
      "stageDuration => 3801 (4 s)\n",
      "executorRunTime => 35848 (36 s)\n",
      "executorCpuTime => 27703 (28 s)\n",
      "executorDeserializeTime => 980 (1.0 s)\n",
      "executorDeserializeCpuTime => 685 (0.7 s)\n",
      "resultSerializationTime => 11 (11 ms)\n",
      "jvmGCTime => 2542 (3 s)\n",
      "shuffleFetchWaitTime => 4 (4 ms)\n",
      "shuffleWriteTime => 1245 (1 s)\n",
      "resultSize => 1964828 (1918.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 2576351232\n",
      "recordsRead => 10024760\n",
      "bytesRead => 715224476 (682.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 9998627\n",
      "shuffleTotalBlocksFetched => 4325\n",
      "shuffleLocalBlocksFetched => 4325\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 34727232 (33.0 MB)\n",
      "shuffleLocalBytesRead => 34727232 (33.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 34727232 (33.0 MB)\n",
      "shuffleRecordsWritten => 9998627\n"
     ]
    }
   ],
   "source": [
    "#QUERY 10\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "tags_and_ratings = ratings.join(tag,on=[\"movieId\"], how='left_outer')\n",
    "\n",
    "df1 = tags_and_ratings\\\n",
    ".select(\"movieId\",\"tag\")\\\n",
    ".where((F.col('tag') == \"funny\") & (F.col('rating') > 3.5)).dropDuplicates()\n",
    "\n",
    "\n",
    "movies_funny = df1.join(movies,on=[\"movieId\"], how='left_outer')\n",
    "\n",
    "\n",
    "\n",
    "df2 = movies_funny.withColumn(\"genres\",explode(split(\"genres\",\"[|]\")))\n",
    "\n",
    "final_df = df2\\\n",
    ".groupby(\"genres\")\\\n",
    ".agg(count(col(\"title\")))\\\n",
    ".withColumnRenamed(\"count(title)\", \"number of movies\")\\\n",
    ".sort(col(\"genres\").asc())\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "altered-shirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa583f200a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-status",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-gasoline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
