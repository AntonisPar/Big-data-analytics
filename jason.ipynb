{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incomplete-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import from_unixtime, hour, dayofyear\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "from sparkmeasure import StageMetrics\n",
    "import numpy as np\n",
    "import random\n",
    "from pyspark.sql.functions import broadcast\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName('Vaseis2')\\\n",
    ".config(\"spark.jars\", \"<path-to-jar>/spark-measure_2.12-0.17.jar\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "stagemetrics = StageMetrics(spark)\n",
    "\n",
    "\n",
    "movies = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"movie.csv\") #filename to read from\n",
    "     )\n",
    "\n",
    "ratings = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"rating.csv\") #filename to read from\n",
    "     )\n",
    "\n",
    "\n",
    "tag = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"tag.csv\") #filename to read from\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "powerful-chassis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 7\n",
      "numTasks => 29\n",
      "elapsedTime => 4354 (4 s)\n",
      "stageDuration => 4261 (4 s)\n",
      "executorRunTime => 46764 (47 s)\n",
      "executorCpuTime => 41818 (42 s)\n",
      "executorDeserializeTime => 145 (0.1 s)\n",
      "executorDeserializeCpuTime => 43 (43 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 1361 (1 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 2 (2 ms)\n",
      "resultSize => 24252 (23.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 20049789\n",
      "bytesRead => 1386473636 (1322.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 12\n",
      "shuffleTotalBlocksFetched => 12\n",
      "shuffleLocalBlocksFetched => 12\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 708 (708 Bytes)\n",
      "shuffleLocalBytesRead => 708 (708 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 708 (708 Bytes)\n",
      "shuffleRecordsWritten => 12\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "stagemetrics.begin()\n",
    "    \n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "movie = movie_df.filter(movie_df[\"title\"] == \"Jumanji (1995)\") #get rows with title jumanji\n",
    "movieID = movie.collect()[0][0] #get movieID\n",
    "\n",
    "q1 = rating_df.filter(rating_df[\"movieId\"] == movieID).select(rating_df[\"userId\"]).count() #count how many people have seen the movie jumanji \n",
    "#output\n",
    "# 22243\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stunning-stomach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "numTasks => 9\n",
      "elapsedTime => 226 (0.2 s)\n",
      "stageDuration => 173 (0.2 s)\n",
      "executorRunTime => 477 (0.5 s)\n",
      "executorCpuTime => 453 (0.5 s)\n",
      "executorDeserializeTime => 28 (28 ms)\n",
      "executorDeserializeCpuTime => 17 (17 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 6 (6 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "resultSize => 9525 (9.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 492846\n",
      "bytesRead => 23677914 (22.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 0\n",
      "shuffleTotalBlocksFetched => 0\n",
      "shuffleLocalBlocksFetched => 0\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 0 (0 Bytes)\n",
      "shuffleLocalBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsWritten => 0\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "stagemetrics.begin()\n",
    "\n",
    "tag_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"tag.csv\") #file to be processed\n",
    ")\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df[\"tag\"].contains('boring')).drop_duplicates(subset=['movieId']).select(\"movieId\")\n",
    "boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stagemetrics.end()\n",
    "\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "encouraging-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|124139|\n",
      "|136015|\n",
      "| 77137|\n",
      "|113181|\n",
      "|124139|\n",
      "| 14517|\n",
      "|  7671|\n",
      "|124139|\n",
      "| 14517|\n",
      "| 20388|\n",
      "| 93037|\n",
      "| 77137|\n",
      "| 79713|\n",
      "|117681|\n",
      "|124139|\n",
      "| 14517|\n",
      "| 62314|\n",
      "| 93037|\n",
      "|117681|\n",
      "|    65|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 9\n",
      "numTasks => 56\n",
      "elapsedTime => 5397 (5 s)\n",
      "stageDuration => 5017 (5 s)\n",
      "executorRunTime => 53683 (54 s)\n",
      "executorCpuTime => 48210 (48 s)\n",
      "executorDeserializeTime => 295 (0.3 s)\n",
      "executorDeserializeCpuTime => 136 (0.1 s)\n",
      "resultSerializationTime => 1 (1 ms)\n",
      "jvmGCTime => 2155 (2 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 70 (70 ms)\n",
      "resultSize => 57492 (56.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 28336128\n",
      "recordsRead => 32661510\n",
      "bytesRead => 1427592728 (1361.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 39\n",
      "shuffleTotalBlocksFetched => 32\n",
      "shuffleLocalBlocksFetched => 32\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 2352 (2.0 KB)\n",
      "shuffleLocalBytesRead => 2352 (2.0 KB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 27891 (27.0 KB)\n",
      "shuffleRecordsWritten => 446\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "stagemetrics.begin()\n",
    "\n",
    "tag_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"tag.csv\") #file to be processed\n",
    ")\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df[\"tag\"] == ('bollywood')).select('userId','movieId')\n",
    "user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), [\"userId\"], 'inner').distinct()\n",
    "user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating[\"rating\"]>3.0).select('userId')\n",
    "\n",
    "user_bollywood_good_rating.show()\n",
    "\n",
    "\n",
    "stagemetrics.end()\n",
    "\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "responsible-investment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|All Passion Spent...|\n",
      "|And Nobody Weeps ...|\n",
      "|Angel and the Bad...|\n",
      "|Another Man's Poi...|\n",
      "|       Aprile (1998)|\n",
      "|     Arranged (2007)|\n",
      "|Asoka (Ashoka the...|\n",
      "|Attack of the Gia...|\n",
      "|    Baby, The (1973)|\n",
      "|Barren Lives (Vid...|\n",
      "|Before the Fall (...|\n",
      "|Better Living Thr...|\n",
      "|Better Than Choco...|\n",
      "| Bikini Beach (1964)|\n",
      "|Bloody Territorie...|\n",
      "|     Bob Funk (2009)|\n",
      "|Bridge of Dragons...|\n",
      "|Brief Vacation, A...|\n",
      "|Brother of Sleep ...|\n",
      "|Cabaret Balkan (B...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 8\n",
      "numTasks => 428\n",
      "elapsedTime => 7370 (7 s)\n",
      "stageDuration => 7120 (7 s)\n",
      "executorRunTime => 79406 (1.3 min)\n",
      "executorCpuTime => 67112 (1.1 min)\n",
      "executorDeserializeTime => 805 (0.8 s)\n",
      "executorDeserializeCpuTime => 585 (0.6 s)\n",
      "resultSerializationTime => 4 (4 ms)\n",
      "jvmGCTime => 5140 (5 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 2029 (2 s)\n",
      "resultSize => 1418618 (1385.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 669057024\n",
      "recordsRead => 40055086\n",
      "bytesRead => 1386473636 (1322.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 1451981\n",
      "shuffleTotalBlocksFetched => 6403\n",
      "shuffleLocalBlocksFetched => 6403\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 19564256 (18.0 MB)\n",
      "shuffleLocalBytesRead => 19564256 (18.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 19564256 (18.0 MB)\n",
      "shuffleRecordsWritten => 1451981\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from pyspark.sql.functions import year, row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')\n",
    "window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())\n",
    "\n",
    "q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])\n",
    "\n",
    "q4_df.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bibliographic-european",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|                tags|\n",
      "+-------+--------------------+--------------------+\n",
      "|  51372|\"\"Great Performan...|                BD-R|\n",
      "|   2072|  'burbs, The (1989)|1980's, black com...|\n",
      "|  69757|(500) Days of Sum...|annoying, artisti...|\n",
      "|  26216|...tick... tick.....|                BD-R|\n",
      "| 128878|            1 (2014)|             Sukumar|\n",
      "|   2572|10 Things I Hate ...|chick flick, Heat...|\n",
      "|  58293|    10,000 BC (2008)|historically inac...|\n",
      "|   6344|101 Reykjavik (10...|             Iceland|\n",
      "|  27251|10th Kingdom, The...|SERIE DE TV, fant...|\n",
      "|  90543|      11 x 14 (1977)|       James Benning|\n",
      "|  90939|11-11-11 (11-11-1...|PG-13:some distur...|\n",
      "|  27674|        11:14 (2003)|multiple storylin...|\n",
      "|   1203| 12 Angry Men (1957)|group psychology,...|\n",
      "|  77846| 12 Angry Men (1997)|             Bob*ola|\n",
      "| 105844|12 Years a Slave ...|based on a book, ...|\n",
      "|  65899|        12:01 (1993)|           time loop|\n",
      "| 128844|     12:01 PM (1990)|Jonathan Heap, ea...|\n",
      "|   7444|13 Going on 30 (2...|Aging, Friends As...|\n",
      "| 127573|     13 Lakes (2004)|       James Benning|\n",
      "|   2826|13th Warrior, The...|             fantasy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 7\n",
      "numTasks => 216\n",
      "elapsedTime => 609 (0.6 s)\n",
      "stageDuration => 476 (0.5 s)\n",
      "executorRunTime => 1628 (2 s)\n",
      "executorCpuTime => 1576 (2 s)\n",
      "executorDeserializeTime => 367 (0.4 s)\n",
      "executorDeserializeCpuTime => 235 (0.2 s)\n",
      "resultSerializationTime => 3 (3 ms)\n",
      "jvmGCTime => 97 (97 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 37 (37 ms)\n",
      "resultSize => 1796915 (1754.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 18874368\n",
      "recordsRead => 985685\n",
      "bytesRead => 47224756 (45.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 5294\n",
      "shuffleTotalBlocksFetched => 1110\n",
      "shuffleLocalBlocksFetched => 1110\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 715392 (698.0 KB)\n",
      "shuffleLocalBytesRead => 715392 (698.0 KB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 715392 (698.0 KB)\n",
      "shuffleRecordsWritten => 5294\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from pyspark.sql.functions import year, concat_ws, collect_list\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "tag_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"tag.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])\n",
    "tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(\", \", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())\n",
    "\n",
    "tag_df_2015.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "oriental-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "numTasks => 15\n",
      "elapsedTime => 2773 (3 s)\n",
      "stageDuration => 2726 (3 s)\n",
      "executorRunTime => 31291 (31 s)\n",
      "executorCpuTime => 27279 (27 s)\n",
      "executorDeserializeTime => 1 (1 ms)\n",
      "executorDeserializeCpuTime => 9 (9 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 1625 (2 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "resultSize => 19435 (18.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 20027545\n",
      "bytesRead => 693302354 (661.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 0\n",
      "shuffleTotalBlocksFetched => 0\n",
      "shuffleLocalBlocksFetched => 0\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 0 (0 Bytes)\n",
      "shuffleLocalBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsWritten => 0\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "stagemetrics.begin()\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "q6_df = rating_df.groupBy('movieId').count()\n",
    "q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adjustable-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "| 72983|\n",
      "| 33736|\n",
      "| 21861|\n",
      "| 13623|\n",
      "|  2261|\n",
      "| 67346|\n",
      "|113991|\n",
      "|134401|\n",
      "| 50943|\n",
      "| 45637|\n",
      "|  8405|\n",
      "| 71975|\n",
      "|118754|\n",
      "| 31404|\n",
      "| 69793|\n",
      "|   741|\n",
      "|120575|\n",
      "| 61168|\n",
      "| 34587|\n",
      "| 91893|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 7\n",
      "numTasks => 250\n",
      "elapsedTime => 5897 (6 s)\n",
      "stageDuration => 5786 (6 s)\n",
      "executorRunTime => 66238 (1.1 min)\n",
      "executorCpuTime => 59529 (60 s)\n",
      "executorDeserializeTime => 137 (0.1 s)\n",
      "executorDeserializeCpuTime => 204 (0.2 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 3072 (3 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 1851 (2 s)\n",
      "resultSize => 797124 (778.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 262144000\n",
      "recordsRead => 40000528\n",
      "bytesRead => 1383420804 (1319.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 223818\n",
      "shuffleTotalBlocksFetched => 3000\n",
      "shuffleLocalBlocksFetched => 3000\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 3717269 (3.0 MB)\n",
      "shuffleLocalBytesRead => 3717269 (3.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 6427667 (6.0 MB)\n",
      "shuffleRecordsWritten => 403620\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import year, row_number, col, count\n",
    "\n",
    "stagemetrics.begin()\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()\n",
    "window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())\n",
    "q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')\n",
    "\n",
    "q7_df.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dress-directive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+---+\n",
      "|  genres|               title|count|  r|\n",
      "+--------+--------------------+-----+---+\n",
      "|   Crime| Pulp Fiction (1994)|67310|  1|\n",
      "|   Crime|Shawshank Redempt...|63366|  2|\n",
      "|   Crime|Silence of the La...|63299|  3|\n",
      "|   Crime|Usual Suspects, T...|47006|  4|\n",
      "|   Crime|       Batman (1989)|46054|  5|\n",
      "| Romance| Forrest Gump (1994)|66172|  1|\n",
      "| Romance|    True Lies (1994)|43159|  2|\n",
      "| Romance|        Speed (1994)|41562|  3|\n",
      "| Romance|Beauty and the Be...|35138|  4|\n",
      "| Romance| Pretty Woman (1990)|33900|  5|\n",
      "|Thriller| Pulp Fiction (1994)|67310|  1|\n",
      "|Thriller|Silence of the La...|63299|  2|\n",
      "|Thriller|Jurassic Park (1993)|59715|  3|\n",
      "|Thriller|  Matrix, The (1999)|51334|  4|\n",
      "|Thriller|Fugitive, The (1993)|49581|  5|\n",
      "|      Fi|Jurassic Park (1993)|59715|  1|\n",
      "|      Fi|Star Wars: Episod...|54502|  2|\n",
      "|      Fi|Terminator 2: Jud...|52244|  3|\n",
      "|      Fi|  Matrix, The (1999)|51334|  4|\n",
      "|      Fi|Independence Day ...|47048|  5|\n",
      "+--------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 15\n",
      "numTasks => 464\n",
      "elapsedTime => 8611 (9 s)\n",
      "stageDuration => 10681 (11 s)\n",
      "executorRunTime => 94952 (1.6 min)\n",
      "executorCpuTime => 75933 (1.3 min)\n",
      "executorDeserializeTime => 882 (0.9 s)\n",
      "executorDeserializeCpuTime => 688 (0.7 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 10310 (10 s)\n",
      "shuffleFetchWaitTime => 2 (2 ms)\n",
      "shuffleWriteTime => 3686 (4 s)\n",
      "resultSize => 1300458 (1269.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 1700265984\n",
      "recordsRead => 40055086\n",
      "bytesRead => 1386473636 (1322.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 20130251\n",
      "shuffleTotalBlocksFetched => 33983\n",
      "shuffleLocalBlocksFetched => 33983\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 77523394 (73.0 MB)\n",
      "shuffleLocalBytesRead => 77523394 (73.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 79493372 (75.0 MB)\n",
      "shuffleRecordsWritten => 20172132\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import year, row_number, col, count, split, explode\n",
    "\n",
    "stagemetrics.begin()\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "q8_df = movie_df.withColumn('genres',explode(split('genres',\"[^\\w]\"))) #seperates rows for each genre in a movie\n",
    "q8_df = q8_df.join(rating_df,['movieId'],'left')\n",
    "q8_df = q8_df.groupBy('genres','title').count()\n",
    "window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())\n",
    "q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)\n",
    "\n",
    "q8_df.show()\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "complete-production",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 2\n",
      "numTasks => 13\n",
      "elapsedTime => 2727 (3 s)\n",
      "stageDuration => 2716 (3 s)\n",
      "executorRunTime => 30408 (30 s)\n",
      "executorCpuTime => 26671 (27 s)\n",
      "executorDeserializeTime => 62 (62 ms)\n",
      "executorDeserializeCpuTime => 22 (22 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 1335 (1 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "resultSize => 19908 (19.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 20000265\n",
      "bytesRead => 691743170 (659.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 0\n",
      "shuffleTotalBlocksFetched => 0\n",
      "shuffleLocalBlocksFetched => 0\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 0 (0 Bytes)\n",
      "shuffleLocalBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsWritten => 0\n"
     ]
    }
   ],
   "source": [
    "9.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour\n",
    "\n",
    "stagemetrics.begin()\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))\n",
    "q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))\n",
    "q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))\n",
    "q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()\n",
    "q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "classical-profit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 6\n",
      "numTasks => 22\n",
      "elapsedTime => 2941 (3 s)\n",
      "stageDuration => 2851 (3 s)\n",
      "executorRunTime => 30012 (30 s)\n",
      "executorCpuTime => 27506 (28 s)\n",
      "executorDeserializeTime => 74 (74 ms)\n",
      "executorDeserializeCpuTime => 34 (34 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 1099 (1 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "resultSize => 19908 (19.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 20493111\n",
      "bytesRead => 715421084 (682.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 0\n",
      "shuffleTotalBlocksFetched => 0\n",
      "shuffleLocalBlocksFetched => 0\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 0 (0 Bytes)\n",
      "shuffleLocalBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsWritten => 0\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, count\n",
    "\n",
    "stagemetrics.begin()\n",
    "rating_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"rating.csv\") #file to be processed\n",
    ")\n",
    "movie_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"movie.csv\") #file to be processed\n",
    ")\n",
    "\n",
    "tag_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option('header','true') #csv files have headers\n",
    "    .option('delimiter', ',') #delimiter of the csv files that are processed\n",
    "    .option('inferSchema', 'true') #if false, all values are strings\n",
    "    .load(\"tag.csv\") #file to be processed\n",
    ")\n",
    "q10_df = movie_df.withColumn('genres',explode(split('genres',\"[^\\w]\"))) #seperates rows for each genre in a movie\n",
    "q10_df = q10_df.join(tag_df,['movieId'],'left')\n",
    "q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df[\"tag\"] == \"funny\")\n",
    "q10_df = q10_df.join(rating_df,['movieId'],'left')\n",
    "q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select(\"*\")\n",
    "q10_df = q10_df.dropDuplicates(['movieId'])\n",
    "q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())\n",
    "\n",
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "threaded-measurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://antonis.station:4048\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://antonis:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd9dcf5bb80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-scout",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-chocolate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-pitch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-hawaii",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-marks",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
